volumes:
  models:

services:
  diarization_service:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "5002:5002"
    volumes:
      - ./settings.json:/app/settings.json:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    stdin_open: true
    tty: true

  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    volumes:
      - ./models:/models
    ports:
      - "1234:1234"
    environment:
      LLAMA_ARG_MODEL: /models/gemma-3-12b-it-q4_0.gguf
      LLAMA_ARG_CTX_SIZE: 32768
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 1234
      LLAMA_ARG_N_GPU_LAYERS: 30
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]